{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hull Tactical Prediction with CNN + Ensemble (LGBM, XGBoost, ENet)\n\n## Algorithm Overview\n\nThis notebook implements a high-performance, hybrid ensemble model for the Hull Tactical Market Prediction competition. It leverages a three-pronged approach:\n\n1.  **Advanced Feature Engineering**: A rich set of features is created, including technical indicators for all base features and non-linear interaction terms.\n2.  **Deep Learning Feature Extraction**: Convolutional Neural Networks (CNNs) are used to automatically extract complex temporal patterns from raw time-series sequences.\n3.  **Triple Model Ensemble**: The final prediction is a weighted average of three distinct models, a strategy proven to be highly effective in competitive machine learning:\n    -   **LightGBM & XGBoost**: Two powerful gradient boosting models trained on the full, combined feature set (technical indicators + interactions + CNN features).\n    -   **ElasticNet**: A robust linear model trained on only the core base features to provide a diversified, stable signal.\n\nThis combination of deep feature engineering and model diversity is designed to maximize predictive accuracy and robustness.","metadata":{}},{"cell_type":"markdown","source":"## 1. Imports and Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport logging\nfrom pathlib import Path\nfrom dataclasses import dataclass, field\n\nimport numpy as np\nimport polars as pl\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport kaggle_evaluation.default_inference_server\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:48:50.751841Z","iopub.execute_input":"2025-09-19T07:48:50.752202Z","iopub.status.idle":"2025-09-19T07:49:05.277547Z","shell.execute_reply.started":"2025-09-19T07:48:50.752177Z","shell.execute_reply":"2025-09-19T07:49:05.275699Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Configuration\n\nThe configuration now includes parameters for all three models (LGBM, XGBoost, ElasticNet) and the weights for combining their predictions in the final ensemble.","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass Config:\n    \"\"\"Configuration for the CNN+Ensemble model.\"\"\"\n    # General & Feature Config\n    sequence_lengths: list = field(default_factory=lambda: [5, 10, 20])\n    max_train_rows: int = None\n    data_dir: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n\n    # CNN Config\n    filters: int = 64\n    kernel_size: int = 3\n    cnn_epochs: int = 15\n    cnn_batch_size: int = 32\n\n    # Ensemble Model Config\n    lgb_n_estimators: int = 500\n    lgb_max_depth: int = 8\n    lgb_learning_rate: float = 0.05\n    xgb_n_estimators: int = 500\n    xgb_max_depth: int = 8\n    xgb_learning_rate: float = 0.05\n    enet_alpha: float = 0.01\n    enet_l1_ratio: float = 0.5\n    validation_size: float = 0.1\n\n    # Ensemble Weights\n    ensemble_weights: dict = field(default_factory=lambda: {'lgb': 0.45, 'xgb': 0.45, 'enet': 0.1})\n\n    # Post-Processing Config\n    vol_window: int = 20\n    signal_multiplier: float = 500.0\n    min_signal: float = 0.0\n    max_signal: float = 2.0\n    vol_scaling: float = 1.0\n    smoothing_factor: float = 0.8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:49:05.279926Z","iopub.execute_input":"2025-09-19T07:49:05.280707Z","iopub.status.idle":"2025-09-19T07:49:05.291073Z","shell.execute_reply.started":"2025-09-19T07:49:05.280646Z","shell.execute_reply":"2025-09-19T07:49:05.289881Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Feature and Model Definitions","metadata":{}},{"cell_type":"code","source":"def create_features(df: pl.DataFrame, feature_cols: list) -> pl.DataFrame:\n    \"\"\"Creates rolling, EWM, and interaction features for all given feature columns.\"\"\"\n    windows = [5, 10, 20]\n    spans = [5, 10, 20]\n    exprs = []\n\n    for col_name in feature_cols:\n        if col_name in df.columns:\n            for w in windows:\n                exprs.append(pl.col(col_name).rolling_mean(w).alias(f'{col_name}_roll_mean_{w}'))\n                exprs.append(pl.col(col_name).rolling_std(w).alias(f'{col_name}_roll_std_{w}'))\n            for s in spans:\n                exprs.append(pl.col(col_name).ewm_mean(span=s).alias(f'{col_name}_ewm_mean_{s}'))\n    \n    # Derived features from reference notebook\n    required_cols = ['I1', 'I2', 'I7', 'I9', 'M11']\n    if all(col in df.columns for col in required_cols):\n        exprs.append((pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"))\n        exprs.append((pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\"U2\"))\n    \n    # Interaction features\n    if 'V1' in df.columns and 'S1' in df.columns:\n        exprs.append((pl.col(\"V1\") * pl.col(\"S1\")).alias(\"V1_S1\"))\n    if 'M11' in df.columns and 'V1' in df.columns:\n        exprs.append((pl.col(\"M11\") * pl.col(\"V1\")).alias(\"M11_V1\"))\n    if 'I9' in df.columns and 'S1' in df.columns:\n        exprs.append((pl.col(\"I9\") * pl.col(\"S1\")).alias(\"I9_S1\"))\n\n    df_with_features = df.with_columns(exprs) if exprs else df\n    return df_with_features.fill_null(strategy=\"forward\").fill_null(strategy=\"backward\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:49:05.292108Z","iopub.execute_input":"2025-09-19T07:49:05.292411Z","iopub.status.idle":"2025-09-19T07:49:05.319187Z","shell.execute_reply.started":"2025-09-19T07:49:05.292387Z","shell.execute_reply":"2025-09-19T07:49:05.317917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNNFeatureExtractor(nn.Module):\n    def __init__(self, sequence_length, num_features, config):\n        super(CNNFeatureExtractor, self).__init__()\n        \n        # 1x1卷积用于特征融合\n        self.feature_fusion = nn.Sequential(\n            nn.Conv1d(in_channels=num_features, out_channels=64, kernel_size=1),\n            nn.BatchNorm1d(64),\n            nn.LeakyReLU()\n        )\n\n        # 堆叠的卷积块\n        self.conv_block1 = nn.Sequential(\n            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=config.kernel_size, padding='same'),\n            nn.BatchNorm1d(128),\n            nn.LeakyReLU(),\n            nn.MaxPool1d(kernel_size=2)\n        )\n        \n        self.conv_block2 = nn.Sequential(\n            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=config.kernel_size, padding='same'),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(),\n            nn.MaxPool1d(kernel_size=2)\n        )\n\n        # 自适应池化层\n        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n        self.flatten = nn.Flatten()\n        \n        # 全连接头，输出10个特征\n        self.fc_head = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)\n        x = self.feature_fusion(x)\n        x = self.conv_block1(x)\n        x = self.conv_block2(x)\n        x = self.adaptive_pool(x)\n        x = self.flatten(x)\n        x = self.fc_head(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:49:05.321787Z","iopub.execute_input":"2025-09-19T07:49:05.322088Z","iopub.status.idle":"2025-09-19T07:49:05.35228Z","shell.execute_reply.started":"2025-09-19T07:49:05.322063Z","shell.execute_reply":"2025-09-19T07:49:05.351303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SharpeLoss(nn.Module):\n    def __init__(self, epsilon=1e-8):\n        super(SharpeLoss, self).__init__()\n        self.epsilon = epsilon\n\n    def forward(self, y_pred, y_true):\n        # 假设y_pred是模型的原始预测（raw predictions）\n        # y_true是真实的目标回报（forward returns）\n        \n        # 在这个竞赛中，一个简单的策略是：预测为正时投入，预测为负时清仓。\n        # 我们将预测值本身看作一个简化的“头寸”或“信号强度”。\n        # 注意：这只是一个代理，真实得分还取决于波动率等。\n        positions = torch.where(y_pred > 0, y_pred, torch.tensor(0.0, device=y_pred.device))\n        \n        # 计算策略回报\n        strategy_returns = positions * y_true\n        \n        # 计算夏普比率\n        mean_return = torch.mean(strategy_returns)\n        std_return = torch.std(strategy_returns)\n        \n        sharpe_ratio = mean_return / (std_return + self.epsilon)\n        \n        # 我们的目标是最大化夏普比率，所以损失是负的夏普比率\n        return -sharpe_ratio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:49:05.353461Z","iopub.execute_input":"2025-09-19T07:49:05.353796Z","iopub.status.idle":"2025-09-19T07:49:05.387234Z","shell.execute_reply.started":"2025-09-19T07:49:05.35376Z","shell.execute_reply":"2025-09-19T07:49:05.385782Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Training Pipeline","metadata":{}},{"cell_type":"markdown","source":"### 4.1. Load and Preprocess Data","metadata":{}},{"cell_type":"code","source":"config = Config()\n\nprint(\"--- Starting Training Pipeline ---\")\n\nprint(\"Loading and preprocessing data...\")\ndf = pl.read_csv(config.data_dir / 'train.csv')\n\ndf = df.with_columns(pl.exclude('date_id').cast(pl.Float64, strict=False))\ndf = df.with_columns(pl.col('date_id').cast(pl.Int32))\n\ndf = df.select([col for col in df.columns if df[col].is_not_null().any()])\ndf = df.with_columns(pl.col('forward_returns').alias('Target'))\n\nif config.max_train_rows:\n    df = df.tail(config.max_train_rows)\n\nprint(f\"Data loading and preprocessing complete. Training on {len(df)} rows.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:49:05.388369Z","iopub.execute_input":"2025-09-19T07:49:05.388658Z","iopub.status.idle":"2025-09-19T07:49:05.824709Z","shell.execute_reply.started":"2025-09-19T07:49:05.388631Z","shell.execute_reply":"2025-09-19T07:49:05.823719Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.2. Create Features and Scale","metadata":{}},{"cell_type":"code","source":"exclude_cols = ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns', 'Target']\nbase_features = [col for col in df.columns if col not in exclude_cols]\ndf_featured = create_features(df, base_features)\n\nall_features = [col for col in df_featured.columns if col not in exclude_cols and col != 'Target']\n\nscaler_full = StandardScaler()\nscaler_base = StandardScaler()\n\ndf_featured_filled = df_featured.fill_nan(None).fill_null(0.0)\n\n# Scale full feature set\ndf_scaled = df_featured_filled.with_columns(pl.from_numpy(scaler_full.fit_transform(df_featured_filled[all_features].to_numpy()), schema=all_features))\n\n# Fit a separate scaler for just the base features for the linear model\nscaler_base.fit(df_featured_filled[base_features].to_numpy())\n\nprint(f\"Feature creation and scaling complete. Total features: {len(all_features)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:49:05.825643Z","iopub.execute_input":"2025-09-19T07:49:05.825959Z","iopub.status.idle":"2025-09-19T07:49:06.383814Z","shell.execute_reply.started":"2025-09-19T07:49:05.825928Z","shell.execute_reply":"2025-09-19T07:49:06.382931Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.3. Train CNNs and Extract Features","metadata":{}},{"cell_type":"code","source":"print(\"Training CNN feature extractors...\")\nall_feature_dfs = {}\ntrained_cnn_extractors = {}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nfor seq_len in config.sequence_lengths:\n    print(f\"\\nGenerating CNN features for sequence length: {seq_len}\")\n    X, y = [], []\n    for i in range(len(df_scaled) - seq_len):\n        X.append(df_scaled[base_features][i:i+seq_len].to_numpy())\n        y.append(df_scaled['Target'][i+seq_len])\n    X, y = np.array(X), np.array(y)\n\n    if len(X) == 0: continue\n\n    X_tensor = torch.tensor(X, dtype=torch.float32)\n    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n    dataset = TensorDataset(X_tensor, y_tensor)\n    loader = DataLoader(dataset, batch_size=config.cnn_batch_size, shuffle=True)\n\n    feature_extractor = CNNFeatureExtractor(seq_len, len(base_features), config)\n    training_model = nn.Sequential(feature_extractor, nn.Linear(10, 1)).to(device)\n    \n    optimizer = torch.optim.Adam(training_model.parameters())\n    criterion = SharpeLoss()\n\n    training_model.train()\n    for epoch in range(config.cnn_epochs):\n        for X_batch, y_batch in loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            \n            # 获取模型预测\n            y_pred = training_model(X_batch)\n            \n            loss = criterion(y_pred, y_batch)\n            loss.backward()\n            optimizer.step()\n\n    feature_extractor.eval()\n    with torch.no_grad():\n        cnn_features = feature_extractor(X_tensor.to(device)).cpu().numpy()\n    \n    all_feature_dfs[seq_len] = pl.DataFrame(cnn_features, schema=[f'cnn_{seq_len}_{i}' for i in range(cnn_features.shape[1])])\n    trained_cnn_extractors[seq_len] = feature_extractor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:49:06.385116Z","iopub.execute_input":"2025-09-19T07:49:06.385456Z","iopub.status.idle":"2025-09-19T07:52:10.287846Z","shell.execute_reply.started":"2025-09-19T07:49:06.385422Z","shell.execute_reply":"2025-09-19T07:52:10.286908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.4. Combine Features for Ensemble Training","metadata":{}},{"cell_type":"code","source":"print(\"Combining features for Ensemble training...\")\nmax_offset = max(config.sequence_lengths)\nfinal_df = df_scaled.slice(max_offset).clone()\n\nfor offset, feature_df in all_feature_dfs.items():\n    start_index = max_offset - offset\n    aligned_feature_df = feature_df.slice(start_index)\n    final_df = final_df.hstack(aligned_feature_df)\n\nensemble_feature_cols = [col for col in final_df.columns if col not in exclude_cols and col != 'Target']\n\nprint(f\"Total features for LGBM/XGBoost: {len(ensemble_feature_cols)}\")\nprint(f\"Total features for ElasticNet: {len(base_features)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:52:10.288927Z","iopub.execute_input":"2025-09-19T07:52:10.28963Z","iopub.status.idle":"2025-09-19T07:52:10.298117Z","shell.execute_reply.started":"2025-09-19T07:52:10.289602Z","shell.execute_reply":"2025-09-19T07:52:10.297027Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.5. Train Ensemble Models","metadata":{}},{"cell_type":"code","source":"print(\"Training Ensemble Models...\")\n# Prepare data for tree models (LGBM, XGB)\nX_ensemble = final_df[ensemble_feature_cols].to_numpy()\ny_ensemble = final_df['Target'].to_numpy()\n\n# Prepare data for linear model (ElasticNet)\nX_base = final_df[base_features].to_numpy()\n\n# Create validation split indices\nval_size = int(len(X_ensemble) * config.validation_size)\ntrain_size = len(X_ensemble) - val_size\n\n# Split full feature set\nX_train_ens, X_val_ens = X_ensemble[:train_size], X_ensemble[train_size:]\ny_train, y_val = y_ensemble[:train_size], y_ensemble[train_size:]\n\n# Split base feature set\nX_train_base, X_val_base = X_base[:train_size], X_base[train_size:]\n\n# --- Train Models ---\nmodels = {}\n\nprint(\"Training LightGBM...\")\nmodels['lgb'] = lgb.LGBMRegressor(n_estimators=config.lgb_n_estimators, max_depth=config.lgb_max_depth, learning_rate=config.lgb_learning_rate, random_state=42, n_jobs=-1)\nmodels['lgb'].fit(X_train_ens, y_train, eval_set=[(X_val_ens, y_val)], eval_metric='rmse', callbacks=[lgb.early_stopping(20, verbose=False)])\n\nprint(\"Training XGBoost...\")\nmodels['xgb'] = xgb.XGBRegressor(n_estimators=config.xgb_n_estimators, max_depth=config.xgb_max_depth, learning_rate=config.xgb_learning_rate, random_state=42, n_jobs=-1)\nmodels['xgb'].fit(X_train_ens, y_train, eval_set=[(X_val_ens, y_val)], verbose=False)\n\nprint(\"Training ElasticNet...\")\n# Scale base features for ElasticNet\nX_train_base_scaled = scaler_base.transform(X_train_base)\nmodels['enet'] = ElasticNet(alpha=config.enet_alpha, l1_ratio=config.enet_l1_ratio)\nmodels['enet'].fit(X_train_base_scaled, y_train)\n\nprint(\"--- Ensemble Training Complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:52:10.300867Z","iopub.execute_input":"2025-09-19T07:52:10.301135Z","iopub.status.idle":"2025-09-19T07:54:42.590854Z","shell.execute_reply.started":"2025-09-19T07:52:10.301113Z","shell.execute_reply":"2025-09-19T07:54:42.588741Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Global State and Inference Function","metadata":{}},{"cell_type":"code","source":"df_train = df_scaled\n\nmax_hist_needed = max(config.sequence_lengths) + config.vol_window\nhistory = df_train.tail(max_hist_needed)\nlast_allocation = 0.0\nhistory = history.with_columns(pl.col(\"date_id\").cast(pl.Int32))\n\nprint(f\"Initial history buffer created with {len(history)} rows.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:54:42.591599Z","iopub.execute_input":"2025-09-19T07:54:42.591889Z","iopub.status.idle":"2025-09-19T07:54:42.601803Z","shell.execute_reply.started":"2025-09-19T07:54:42.591865Z","shell.execute_reply":"2025-09-19T07:54:42.600804Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Helper Function for Inference","metadata":{}},{"cell_type":"code","source":"def prepare_prediction_features(test_df: pl.DataFrame, history_df: pl.DataFrame) -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n    \"\"\"Prepares feature sets for all models for a single test row.\"\"\"\n    # 1. Align test row schema\n    test_df = test_df.with_columns(pl.exclude('date_id').cast(pl.Float64, strict=False))\n    test_df = test_df.with_columns(pl.col('date_id').cast(pl.Int32))\n\n    required_cols = history_df.columns\n    aligned_test_df = pl.DataFrame({col: [None] for col in required_cols}).cast({c: history_df[c].dtype for c in required_cols})\n    for col in test_df.columns:\n        if col in aligned_test_df.columns:\n            aligned_test_df = aligned_test_df.with_columns(pl.lit(test_df[col].item()).alias(col))\n\n    # 2. Combine with history\n    combined_df = pl.concat([history_df, aligned_test_df])\n\n    # 3. Create features\n    combined_df_featured = create_features(combined_df, base_features)\n    combined_df_featured = combined_df_featured.fill_nan(None).fill_null(strategy=\"forward\").fill_null(strategy=\"backward\").fill_null(0.0)\n\n    # 4. Scale features\n    df_scaled_full = combined_df_featured.with_columns(pl.from_numpy(scaler_full.transform(combined_df_featured[all_features].to_numpy()), schema=all_features))\n    df_scaled_base = combined_df_featured.with_columns(pl.from_numpy(scaler_base.transform(combined_df_featured[base_features].to_numpy()), schema=base_features))\n\n    # 5. Generate CNN features\n    max_offset = max(config.sequence_lengths)\n    cnn_input_df = df_scaled_full.tail(max_offset)\n    test_cnn_features_list = []\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    for seq_len in config.sequence_lengths:\n        cnn_model = trained_cnn_extractors[seq_len]\n        sequence = cnn_input_df.tail(seq_len).select(base_features).to_numpy()\n        sequence = np.expand_dims(sequence, axis=0)\n        X_test_tensor = torch.tensor(sequence, dtype=torch.float32).to(device)\n        with torch.no_grad():\n            features = cnn_model(X_test_tensor).cpu().numpy()\n        test_cnn_features_list.append(pl.DataFrame(features, schema=[f'cnn_{seq_len}_{i}' for i in range(features.shape[1])]))\n    \n    df_test_cnn_features = pl.concat(test_cnn_features_list, how='horizontal')\n\n    # 6. Create final feature sets for each model type\n    latest_other_features = df_scaled_full.tail(1).select(all_features)\n    latest_other_features = latest_other_features.select([col for col in latest_other_features.columns if not col.startswith('cnn_')])\n    df_test_ensemble_features = latest_other_features.hstack(df_test_cnn_features).select(ensemble_feature_cols)\n    \n    df_test_base_features = df_scaled_base.tail(1).select(base_features)\n    \n    return df_test_ensemble_features, df_test_base_features, aligned_test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:54:42.602922Z","iopub.execute_input":"2025-09-19T07:54:42.60345Z","iopub.status.idle":"2025-09-19T07:54:42.628356Z","shell.execute_reply.started":"2025-09-19T07:54:42.603416Z","shell.execute_reply":"2025-09-19T07:54:42.627315Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prediction Function with Ensemble and Post-Processing","metadata":{}},{"cell_type":"code","source":"def predict(test_df: pl.DataFrame) -> float:\n    \"\"\"Makes a prediction for a single new row using an ensemble model.\"\"\"\n    global history, last_allocation\n\n    # 1. Generate features for all models\n    df_ens, df_base, aligned_test_df = prepare_prediction_features(test_df, history)\n\n    # 2. Get predictions from each model\n    preds = {}\n    preds['lgb'] = models['lgb'].predict(df_ens.to_numpy())[0]\n    preds['xgb'] = models['xgb'].predict(df_ens.to_numpy())[0]\n    preds['enet'] = models['enet'].predict(df_base.to_numpy())[0]\n\n    # 3. Combine predictions using weights\n    raw_pred = sum(preds[name] * config.ensemble_weights[name] for name in models)\n    \n    # 4. Apply post-processing\n    recent_returns = history['Target'].tail(config.vol_window).to_numpy()\n    volatility = np.std(recent_returns) if len(recent_returns) > 1 else 0.01\n    volatility = max(volatility, 0.01)\n    \n    signal = np.clip(raw_pred * config.signal_multiplier, config.min_signal, config.max_signal)\n    allocation = np.clip(signal / (volatility * config.vol_scaling), config.min_signal, config.max_signal)\n    \n    final_allocation = (config.smoothing_factor * allocation + \n                       (1 - config.smoothing_factor) * last_allocation)\n    \n    # 5. Update state for the next iteration\n    last_allocation = final_allocation\n    new_history_row = aligned_test_df.with_columns(pl.lit(float(raw_pred)).alias('Target'))\n    history = pl.concat([history.tail(len(history) - 1), new_history_row])\n\n    return float(final_allocation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:54:42.629505Z","iopub.execute_input":"2025-09-19T07:54:42.630167Z","iopub.status.idle":"2025-09-19T07:54:42.65703Z","shell.execute_reply.started":"2025-09-19T07:54:42.630135Z","shell.execute_reply":"2025-09-19T07:54:42.656189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the entire test file\n# test_df = pl.read_csv(config.data_dir / \"test.csv\")\n\n# # Iterate through the test DataFrame row by row\n# print(\"--- Running local test prediction loop ---\")\n# for i in range(len(test_df)):\n#     # Get the i-th row as a new single-row DataFrame\n#     single_row_df = test_df.slice(i, 1)\n    \n#     # Call the predict function with the single-row DataFrame\n#     prediction = predict(single_row_df)\n    \n#     print(f\"Prediction for row {i}: {prediction}\")\n\n# print(\"--- Local test complete ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:54:42.659434Z","iopub.execute_input":"2025-09-19T07:54:42.659782Z","iopub.status.idle":"2025-09-19T07:54:42.682699Z","shell.execute_reply.started":"2025-09-19T07:54:42.659758Z","shell.execute_reply":"2025-09-19T07:54:42.681613Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Server Execution","metadata":{}},{"cell_type":"code","source":"inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    logger.info(\"Starting inference server in competition mode\")\n    inference_server.serve()\nelse:\n    logger.info(\"Running local gateway for testing\")\n    inference_server.run_local_gateway((str(config.data_dir),))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-19T07:54:42.684035Z","iopub.execute_input":"2025-09-19T07:54:42.684336Z","iopub.status.idle":"2025-09-19T07:54:44.736133Z","shell.execute_reply.started":"2025-09-19T07:54:42.684313Z","shell.execute_reply":"2025-09-19T07:54:44.735082Z"}},"outputs":[],"execution_count":null}]}